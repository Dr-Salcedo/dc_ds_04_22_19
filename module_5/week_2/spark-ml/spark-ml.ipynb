{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Machine learning \n",
    "\n",
    "### Learning goals:\n",
    "- align the relationships between Hadoop, Spark, and Databricks\n",
    "- differentiate between Spark RDDs and Spark Dataframes and when each is appropriate\n",
    "- locate and explore the Spark.ML documentation\n",
    "- code along a text classification problem using four different ml algorithms, a data prep pipeline, and gridsearch to fine tune a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark context and concepts review\n",
    "![sparkler](https://images.pexels.com/photos/285173/pexels-photo-285173.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The story of Spark (in diagrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Start with Hadoop\n",
    "![diagram of hadoop v1 compared to hadoop v2](img/yarn.png)\n",
    "[diagram source](https://sites.google.com/site/codingbughunter/hadoop/yarn-general-discribe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Yarn facilitates the resource allocation between Spark and the HDFS\n",
    "### YARN = Yet Another Resource Negotiator\n",
    "#### YARN is a subproduct of Hadoop\n",
    "![yarn diagram with spark](http://hortonworks.com/wp-content/uploads/2013/06/YARN.png)\n",
    "\n",
    "[diagram source](https://sites.google.com/site/codingbughunter/hadoop/yarn-general-discribe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Then visualize the Spark ecosystem built on top of that\n",
    "\n",
    "![diagram of spark eco system components](img/spark_eco.png)\n",
    "\n",
    "[image source here](https://databricks.com/spark/about)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Databricks provides wrap around services around _that_\n",
    "![databricks architecture diagram](img/Databricks_product.png)\n",
    "[diagram source](https://verify.wiki/wiki/Databricks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The story of Spark (a timeline)\n",
    "\n",
    "|<p align=\"left justify\">Date</p>|<p align=\"left justify\">Product</p>|<p align=\"left justify\">Update</p>|\n",
    "|:----|:-----|:-----|\n",
    "| 2002 | Hadoop | <p align=\"left justify\">Doug Cutting starts `Apache Nutch` researching sort/merge processing</p> |\n",
    "| 2006 | Hadoop |  <p align=\"left justify\">Leaves `Nutch` and joins `Yahoo`, renaming the project `Hadoop` </p>|\n",
    "| 2008 | Hadoop |  <p align=\"left justify\">`Hadoop` was made `Apache’s` top level project </p> |\n",
    "| Jan 2008 | Hadoop |  <p align=\"left justify\">v 0.10.1 released </p>|\n",
    "| 2009 | Spark | <p align=\"left justify\">started as a research project at the UC Berkeley AMPLab  </p>|\n",
    "| 2010 | Spark |  <p align=\"left justify\">open sourced </p>|\n",
    "| Sept 2012 | Spark |  <p align=\"left justify\">0.6.0 released </p>|\n",
    "| 2013 | Spark |  <p align=\"left justify\">moved to the `Apache` Software Foundation </p>|\n",
    "| Feb 2013| Spark |  <p align=\"left justify\">Spark 0.7 adds a Python API called `PySpark` </p>|\n",
    "| Sept 2013 | Spark | <p align=\"left justify\">0.8.0 introduces `MLlib` </p>|\n",
    "| 2013 | Databricks |  <p align=\"left justify\">Original Spark research team at UC Berkeley found Databricks</p> |\n",
    "| May 2014 |Spark |  <p align=\"left justify\">v 1.0 introduces Spark SQL, for loading and manipulating structured data in Spark</p>|\n",
    "| Sept 2014 | Spark|  <p align=\"left justify\">v 1.1.0 provided support for registering Python lambda funtions as UDFs</p>|\n",
    "|Mar 2015 | Spark | <p align=\"left justify\"> v 1.3.0 brings a new DataFrame API</p> |\n",
    "| Jun 2015 | Spark | <p align=\"left justify\"> v 1.4.0 brings an R API to Spark</p> |\n",
    "| 2015 | Databricks | <p align=\"left justify\"> The Databricks Apache Spark cloud platform goes public</p> |\n",
    "| Jan 2016|  Spark | <p align=\"left justify\"> v 1.6.0 brings a new Dataset API <br> - A new Spark API, similar to RDDs, that allows users to work with custom objects and lambda functions while still gaining the benefits of the Spark SQL execution engine.</p> |\n",
    "| Jul 2016 | Spark | <p align=\"left justify\"> v 2.0.0 **big update**! <Br> - Unifying DataFrame and Dataset: In Scala and Java, DataFrame and Dataset have been unified, i.e. DataFrame is just a type alias for Dataset of Row. In Python and R, given the lack of type safety, DataFrame is the main programming interface. <br> - SparkSession: new entry point that replaces the old SQLContext<br>- Native CSV data source, based on Databricks’ spark-csv module<br>- MLlib - The DataFrame-based API is now the primary API. The RDD-based API is entering maintenance mode </p> |\n",
    "| 2016 | Databricks | <p align=\"left justify\"> Databricks Launches Free Community Edition As Companion To Free Online Spark Courses </p>|\n",
    "| Jul 2017| Spark | <p align=\"left justify\"> v 2.2.0 drops support for Python 2.6 |\n",
    "| Nov 2018 | Spark | <p align=\"left justify\"> v 2.4.0<br> - This release adds Barrier Execution Mode for better integration with deep learning frameworks<br> - more integration between pandas UDF and spark DataFrames </p>|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark data objects\n",
    "\n",
    "![diagram of definitions of Spark objects from databricks](https://databricks.com/wp-content/uploads/2018/05/rdd-1024x595.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In Pyspark there are only RDD and DataFrames\n",
    "\n",
    "In other languages where \"compiling\" is done, there is the distinction between DataFrames and DataSet. \n",
    "\n",
    "![dataframe image](https://databricks.com/wp-content/uploads/2018/05/DataFrames.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Differences between objects:\n",
    "![memory usage](https://databricks.com/wp-content/uploads/2016/07/memory-usage-when-caching-datasets-vs-rdds.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Use an RDD when:\n",
    "[quoted from databricks](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)\n",
    "\n",
    "> - you want low-level transformation and actions and control on your dataset;\n",
    "> - your data is unstructured, such as media streams or streams of text;\n",
    "> - you want to manipulate your data with functional programming constructs than domain specific expressions;\n",
    "> - you don’t care about imposing a schema, such as columnar format, while processing or accessing data attributes by name or column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Use a dataframe when:\n",
    "[also quoted from databricks](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)\n",
    "\n",
    "\n",
    "> - you want rich semantics, high-level abstractions, and domain specific APIs, use DataFrame\n",
    "> - your processing demands high-level expressions, filters, maps, aggregation, averages, sum, SQL queries, columnar access and use of lambda functions on semi-structured data, use DataFrame\n",
    "> - you want higher degree of type-safety at compile time, want typed JVM objects, take advantage of Catalyst optimization, and benefit from Tungsten’s efficient code generation, use Dataset.\n",
    "> - you want unification and simplification of APIs across Spark Libraries, use DataFrame or Dataset.\n",
    "> - If you are a R user, use DataFrames.\n",
    "> - If you are a Python user, use DataFrames and resort back to RDDs if you need more control.\n",
    "\n",
    "**Note**: Machine learning algorithms are run on _DataFrames_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review:\n",
    "\n",
    "- You are grabbing live tweets about the CW show 'Jane the Virgin' for later analysis. In the Spark ecosystem, where should you store them? an RDD or a DataFrame?\n",
    "\n",
    "- You have an RDD of data that you wish to use to build a predictive model. Should you leave it as an RDD or transform it to a DataFrame?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine learning in Spark\n",
    "![bbc logo](https://www.nwcu.police.uk/wp-content/uploads/2013/05/BBC-News.png)\n",
    "\n",
    "Section influenced by [this analysis of twitter data](https://wesslen.github.io/twitter/predicting_twitter_profile_location_with_pyspark/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The return of Greg\n",
    "\n",
    "![greg](img/thinking.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Greg's life is full of pain\n",
    "\n",
    "Greg has become really tired of his boss asking him to do all these random things.<br>\n",
    "**First** she had him learn Object Oriented Programming and it's been down hill ever since.<br>\n",
    "**Now** she's wanting him to send her a summary of political news from the BBC each day.<br>\n",
    "The problem is it takes him hours just to sort through the BBC website to get *just* the political articles that interest her."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## But wait!\n",
    "What if rather than sorting through them himself he could build a classification model that will sort only the ones he needs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Create spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:22:26.993425Z",
     "start_time": "2019-07-01T18:22:20.642453Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:22:55.456938Z",
     "start_time": "2019-07-01T18:22:54.345596Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.113.105.157:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x11d862a90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:22:55.482455Z",
     "start_time": "2019-07-01T18:22:55.474917Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.113.105.157:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Read in our dataset of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:26:59.859547Z",
     "start_time": "2019-07-01T18:26:59.259159Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bbc = spark.read.csv(path='bbc-text.csv',sep=',',encoding='UTF-8', header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:26:59.895063Z",
     "start_time": "2019-07-01T18:26:59.700Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show(df, n=5):\n",
    "    return df.limit(n).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Do some basic data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:27:00.693855Z",
     "start_time": "2019-07-01T18:27:00.686363Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['category', 'text']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:27:01.273767Z",
     "start_time": "2019-07-01T18:27:01.266348Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('category', 'string'), ('text', 'string')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:27:01.527317Z",
     "start_time": "2019-07-01T18:27:01.521250Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bbc.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:27:01.929517Z",
     "start_time": "2019-07-01T18:27:01.756540Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>politics</td>\n",
       "      <td>howard hits back at mongrel jibe michael howar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>politics</td>\n",
       "      <td>blair prepares to name poll date tony blair is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sport</td>\n",
       "      <td>henman hopes ended in dubai third seed tim hen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sport</td>\n",
       "      <td>wilkinson fit to face edinburgh england captai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>last star wars  not for children  the sixth an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve...\n",
       "5       politics  howard hits back at mongrel jibe michael howar...\n",
       "6       politics  blair prepares to name poll date tony blair is...\n",
       "7          sport  henman hopes ended in dubai third seed tim hen...\n",
       "8          sport  wilkinson fit to face edinburgh england captai...\n",
       "9  entertainment  last star wars  not for children  the sixth an..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:27:02.287739Z",
     "start_time": "2019-07-01T18:27:02.110891Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2225"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:27:03.835001Z",
     "start_time": "2019-07-01T18:27:02.438235Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|     category|count|\n",
      "+-------------+-----+\n",
      "|        sport|  511|\n",
      "|     politics|  417|\n",
      "|entertainment|  386|\n",
      "|     business|  510|\n",
      "|         tech|  401|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bbc.groupBy('category').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:27:05.176331Z",
     "start_time": "2019-07-01T18:27:05.082368Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a new column of target \"politics\"\n",
    "from pyspark.sql.functions import when, col\n",
    "bbc = bbc.withColumn(\"label\", \\\n",
    "                           (when(col(\"category\").like(\"%politics%\"), 1) \\\n",
    "                           .otherwise(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:27:07.769411Z",
     "start_time": "2019-07-01T18:27:07.757721Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# drop original target column\n",
    "bbc = bbc.drop(bbc.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:23:16.109313Z",
     "start_time": "2019-07-01T18:23:15.849274Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>howard hits back at mongrel jibe michael howar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>blair prepares to name poll date tony blair is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>henman hopes ended in dubai third seed tim hen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wilkinson fit to face edinburgh england captai...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>last star wars  not for children  the sixth an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  tv future in the hands of viewers with home th...      0\n",
       "1  worldcom boss  left books alone  former worldc...      0\n",
       "2  tigers wary of farrell  gamble  leicester say ...      0\n",
       "3  yeading face newcastle in fa cup premiership s...      0\n",
       "4  ocean s twelve raids box office ocean s twelve...      0\n",
       "5  howard hits back at mongrel jibe michael howar...      1\n",
       "6  blair prepares to name poll date tony blair is...      1\n",
       "7  henman hopes ended in dubai third seed tim hen...      0\n",
       "8  wilkinson fit to face edinburgh england captai...      0\n",
       "9  last star wars  not for children  the sixth an...      0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(bbc,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine LEarning in Spark\n",
    "\n",
    "Spark's [documentation](https://spark.apache.org/docs/2.2.0/ml-guide.html#mllib-main-guide) is fairly straight forward!  Let's take a look. It shouldn't look *too* different than `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data prep pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:35:59.092561Z",
     "start_time": "2019-07-01T18:35:58.428254Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "\n",
    "\n",
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# stop words\n",
    "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"can\"] # standard stop words\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "\n",
    "# bag of words count\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:36:43.811172Z",
     "start_time": "2019-07-01T18:36:40.783471Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(bbc)\n",
    "dataset = pipelineFit.transform(bbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:36:57.353690Z",
     "start_time": "2019-07-01T18:36:53.065153Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 1561\n",
      "Test Dataset Count: 664\n"
     ]
    }
   ],
   "source": [
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:38:44.396574Z",
     "start_time": "2019-07-01T18:38:38.923298Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "# Build the model\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0, family = \"binomial\")\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:38:44.486626Z",
     "start_time": "2019-07-01T18:38:39.415Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "beta = np.sort(lrModel.coefficients)\n",
    "\n",
    "plt.plot(beta)\n",
    "plt.ylabel('Beta Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary has many components one can call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:38:44.493498Z",
     "start_time": "2019-07-01T18:38:40.556Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract the summary from the returned LogisticRegressionModel instance trained\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "plt.plot(objectiveHistory)\n",
    "plt.ylabel('Objective Function')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:38:44.501390Z",
     "start_time": "2019-07-01T18:38:41.166Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n",
    "print(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))\n",
    "\n",
    "#trainingSummary.roc.show(n=10, truncate=15)\n",
    "roc = trainingSummary.roc.toPandas()\n",
    "plt.plot(roc['FPR'],roc['TPR'])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:38:44.516383Z",
     "start_time": "2019-07-01T18:38:41.692Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pr = trainingSummary.pr.toPandas()\n",
    "plt.plot(pr['recall'],pr['precision'])\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:41:00.808312Z",
     "start_time": "2019-07-01T18:41:00.793422Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainingSummary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-b84848cef2a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Set the model threshold to maximize F-Measure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#trainingSummary.fMeasureByThreshold.show(n=10, truncate = 15)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainingSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfMeasureByThreshold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'threshold'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'F-Measure'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'F-Measure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainingSummary' is not defined"
     ]
    }
   ],
   "source": [
    "# Set the model threshold to maximize F-Measure\n",
    "#trainingSummary.fMeasureByThreshold.show(n=10, truncate = 15)\n",
    "f = trainingSummary.fMeasureByThreshold.toPandas()\n",
    "plt.plot(f['threshold'],f['F-Measure'])\n",
    "plt.ylabel('F-Measure')\n",
    "plt.xlabel('Threshold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:40:53.058280Z",
     "start_time": "2019-07-01T18:40:51.449131Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------+\n",
      "|                                    text|                             probability|\n",
      "+----------------------------------------+----------------------------------------+\n",
      "|2d metal slug offers retro fun like s...|[0.9804110787644086,0.019588921235591...|\n",
      "|a year to remember for irish there us...|[0.9971249956337924,0.002875004366207...|\n",
      "|a-listers flock to gervais sitcom hol...|[0.9802504324750112,0.01974956752498866]|\n",
      "|aaliyah claim dismissed by court late...|[0.9607748047895676,0.039225195210432...|\n",
      "|actress roberts takes spider role act...|[0.9728119265594252,0.027188073440574...|\n",
      "|adventure tale tops awards young book...|[0.9765669471450193,0.02343305285498065]|\n",
      "|african double in edinburgh world 500...|[0.9705370095883613,0.02946299041163871]|\n",
      "|aids and climate top davos agenda cli...|[0.45518147294539374,0.5448185270546063]|\n",
      "|ajax refuse to rule out jol move ajax...|[0.981080418156163,0.018919581843837074]|\n",
      "|alfa romeos  to get gm engines  fiat ...|[0.9471905771345527,0.05280942286544731]|\n",
      "+----------------------------------------+----------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test data using the transform() method.\n",
    "# LogisticRegression.transform() will only use the 'features' column.\n",
    "predictions = lrModel.transform(testData)\n",
    "\n",
    "predictions.select(\"text\",\"probability\").show(n=10, truncate=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Prediction object is a dataframe\n",
    "with some options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:41:20.162721Z",
     "start_time": "2019-07-01T18:41:20.155703Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: integer (nullable = false)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:41:23.007685Z",
     "start_time": "2019-07-01T18:41:20.876266Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------------------------+-----+----------+\n",
      "|                          text|                   probability|label|prediction|\n",
      "+------------------------------+------------------------------+-----+----------+\n",
      "|uk youth  interested  in po...|[0.4920116864757073,0.50798...|    1|       1.0|\n",
      "|nuclear dumpsite  plan atta...|[0.48730602499976905,0.5126...|    1|       1.0|\n",
      "|end bush  denial  blair tel...|[0.4699202484721018,0.53007...|    1|       1.0|\n",
      "|tory  stalking horse  meyer...|[0.4675499884893308,0.53245...|    1|       1.0|\n",
      "|labour trio  had vote-rig f...|[0.4661237332865865,0.53387...|    1|       1.0|\n",
      "|aids and climate top davos ...|[0.45518147294539374,0.5448...|    0|       1.0|\n",
      "|games enter the classroom v...|[0.4511822517208836,0.54881...|    0|       1.0|\n",
      "|uk  needs true immigration ...|[0.45042819009705903,0.5495...|    1|       1.0|\n",
      "|few ready  for information ...|[0.44380016309838305,0.5561...|    1|       1.0|\n",
      "|kennedy predicts bigger tur...|[0.4402414376855826,0.55975...|    1|       1.0|\n",
      "|former ni minister scott di...|[0.4241213309904169,0.57587...|    1|       1.0|\n",
      "|brown visits slum on africa...|[0.41752355845175526,0.5824...|    1|       1.0|\n",
      "|bid to cut court witness st...|[0.4119410001340259,0.58805...|    1|       1.0|\n",
      "|blair hails turkey-eu talks...|[0.40900799518871117,0.5909...|    1|       1.0|\n",
      "|parents face video game les...|[0.40633631083932276,0.5936...|    0|       1.0|\n",
      "|jamieson issues warning to ...|[0.392795652608944,0.607204...|    1|       1.0|\n",
      "|baa support ahead of court ...|[0.37812777289636906,0.6218...|    1|       1.0|\n",
      "|no-one can define new hunt ...|[0.3574256540616796,0.64257...|    1|       1.0|\n",
      "|parties warned over  grey v...|[0.34863242593699967,0.6513...|    1|       1.0|\n",
      "|hague s six-figure earnings...|[0.34727187180452285,0.6527...|    1|       1.0|\n",
      "+------------------------------+------------------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.filter(predictions['prediction'] == 1) \\\n",
    "    .select(\"text\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 20, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:41:23.088560Z",
     "start_time": "2019-07-01T18:41:21.157Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "print(\"Training: Area Under ROC: \" + str(trainingSummary.areaUnderROC))\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print(\"Test: Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes\n",
    "#### Specify and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:41:23.096932Z",
     "start_time": "2019-07-01T18:41:21.624Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "nb = NaiveBayes(smoothing=1, modelType=\"multinomial\")\n",
    "\n",
    "# train the model\n",
    "model = nb.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Evaluate Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:41:23.103277Z",
     "start_time": "2019-07-01T18:41:22.448Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select example rows to display.\n",
    "predictions = model.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 1) \\\n",
    "    .select(\"text\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 20, truncate = 30)\n",
    "\n",
    "# compute accuracy on the test set\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "print(\"Test: Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision Tree\n",
    "#### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:41:23.111531Z",
     "start_time": "2019-07-01T18:41:22.907Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Create initial Decision Tree Model\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=3)\n",
    "\n",
    "# Train model with Training Data\n",
    "dtModel = dt.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:41:23.253112Z",
     "start_time": "2019-07-01T18:41:23.234658Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dtModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-71318f4c7a4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"numNodes = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumNodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"depth = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dtModel' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"numNodes = \", dtModel.numNodes)\n",
    "print( \"depth = \", dtModel.depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Evaluate Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:41:23.500635Z",
     "start_time": "2019-07-01T18:41:23.484759Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dtModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-2067957cea17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"probability\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"probability\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dtModel' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = dtModel.transform(testData)\n",
    "\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"text\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:41:23.770784Z",
     "start_time": "2019-07-01T18:41:23.755063Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BinaryClassificationEvaluator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-58ab8d7208d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBinaryClassificationEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawPredictionCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rawPrediction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test: Area Under ROC: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetricName\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"areaUnderROC\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BinaryClassificationEvaluator' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print(\"Test: Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "#### Specify model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:41:38.578240Z",
     "start_time": "2019-07-01T18:41:23.882313Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Create an initial RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 100, \\\n",
    "                            maxDepth = 4, \\\n",
    "                            maxBins = 32)\n",
    "\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Score and evaluate Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:41:38.613744Z",
     "start_time": "2019-07-01T18:41:25.417Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Score test Data\n",
    "predictions = rfModel.transform(testData)\n",
    "\n",
    "predictions.filter(predictions['prediction'] == 1) \\\n",
    "    .select(\"text\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:41:38.620916Z",
     "start_time": "2019-07-01T18:41:26.054Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print(\"Test: Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementing grid search with `CrossValidator` in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T18:41:38.627682Z",
     "start_time": "2019-07-01T18:41:27.182Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.numTrees, [50, 100, 200]) # number of trees\n",
    "             .addGrid(rf.maxDepth, [3, 4, 5]) # maximum depth\n",
    "#            .addGrid(rf.maxBins, [24, 32, 40]) #Number of bins\n",
    "             .build())\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=rf, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(trainingData)\n",
    "\n",
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "predictions = cvModel.transform(testData)\n",
    "\n",
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "print(\"Test: Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Which model had the best AUC?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning goals in review. How did we do?\n",
    "- align the relationships between Hadoop, Spark, and Databricks\n",
    "- differentiate between Spark RDDs and Spark Dataframes and when each is appropriate\n",
    "- locate and explore the Spark.ML documentation\n",
    "- code along a text classification problem using four different ml algorithms, a data prep pipeline, and gridsearch to fine tune a model"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
