{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Mining and NLP\n",
    "\n",
    "## Part 2\n",
    "\n",
    "### Situation:\n",
    "\n",
    "Priya works at an international PR firm in the Europe division. Their largest client has offices in Ibiza, Madrid, and Las Palmas. She needs to keep her boss aware of current events and provide a weekly short list of articles concerning political events in Spain. The problem is, this takes hours every week to review articles on the BBC and Priya is very busy! She wonders if she could automate this process using text mining to save her time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Goal**: to internalize the steps, challenges, and methodology of text mining\n",
    "- explore text analysis by hand\n",
    "- apply text mining steps in Jupyter with Python libraries NLTK\n",
    "- classify documents correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refresher on cleaning text\n",
    "![gif](https://www.nyfa.edu/student-resources/wp-content/uploads/2014/10/furious-crazed-typing.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T19:04:30.639888Z",
     "start_time": "2019-07-09T19:04:30.460143Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import nltk\n",
    "import sklearn\n",
    "\n",
    "from nltk.collocations import *\n",
    "from nltk import FreqDist, word_tokenize\n",
    "import string, re\n",
    "import urllib\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "url_a = \"https://raw.githubusercontent.com/aapeebles/text_examples/master/Text%20examples%20folder/A.txt\"\n",
    "url_b = \"https://raw.githubusercontent.com/aapeebles/text_examples/master/Text%20examples%20folder/B.txt\"\n",
    "article_a = urllib.request.urlopen(url_a).read()\n",
    "article_a_st = article_a.decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T19:04:32.194503Z",
     "start_time": "2019-07-09T19:04:32.175481Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokens\n",
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "arta_tokens_raw = nltk.regexp_tokenize(article_a_st, pattern)\n",
    "\n",
    "# lower case\n",
    "arta_tokens = [i.lower() for i in arta_tokens_raw]\n",
    "\n",
    "# stop words\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words(\"english\")\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "arta_tokens_stopped = [w for w in arta_tokens if not w in stop_words]\n",
    "\n",
    "# stem words\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "arta_stemmed = [stemmer.stem(word) for word in arta_tokens_stopped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T19:04:39.158755Z",
     "start_time": "2019-07-09T19:04:39.079728Z"
    }
   },
   "outputs": [],
   "source": [
    "# repeat w second article\n",
    "article_b = urllib.request.urlopen(url_b).read()\n",
    "article_b_st = article_b.decode(\"utf-8\")\n",
    "artb_tokens_raw = nltk.regexp_tokenize(article_b_st, pattern)\n",
    "artb_tokens = [i.lower() for i in artb_tokens_raw]\n",
    "artb_tokens_stopped = [w for w in artb_tokens if not w in stop_words]\n",
    "artb_stemmed = [stemmer.stem(word) for word in artb_tokens_stopped]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what's wrong with the table from yesterday? what does it not consider?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency (TF)\n",
    "\n",
    "$\\begin{align}\n",
    " tf_{i,j} = \\dfrac{n_{i,j}}{\\displaystyle \\sum_k n_{i,j} }\n",
    "\\end{align} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency (IDF)\n",
    "\n",
    "$\\begin{align}\n",
    "idf(w) = \\log \\dfrac{N}{df_t}\n",
    "\\end{align} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF score\n",
    "\n",
    "$ \\begin{align}\n",
    "w_{i,j} = tf_{i,j} \\times \\log \\dfrac{N}{df_i} \\\\\n",
    "tf_{i,j} = \\text{number of occurences of } i \\text{ in} j \\\\\n",
    "df_i = \\text{number of documents containing} i \\\\\n",
    "N = \\text{total number of documents}\n",
    "\\end{align} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The from scratch method\n",
    "![homemade](https://media2.giphy.com/media/LBZcXdG0eVBdK/giphy.gif?cid=3640f6095c2d7bb2526a424a4d97117c)\n",
    "\n",
    "\n",
    "Please go through the code and comment what each section does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T19:04:42.040009Z",
     "start_time": "2019-07-09T19:04:42.029925Z"
    }
   },
   "outputs": [],
   "source": [
    "wordSet = set(arta_stemmed).union(set(artb_stemmed)) #get all unique words (no dupliates) from both datasets and join them into a word set\n",
    "wordDictA = dict.fromkeys(wordSet, 0)  #assigning a value of 0 to all keys in dictionary \n",
    "wordDictB = dict.fromkeys(wordSet, 0)  #assigning a value of 0 to all keys in dictionary \n",
    "\n",
    "for word in arta_stemmed:  #increase value counter by 1 if word match dict key \n",
    "    wordDictA[word]+=1\n",
    "    \n",
    "for word in artb_stemmed: ##increase value counter by 1 if word match dict key \n",
    "    wordDictB[word]+=1    \n",
    "\n",
    "def computeTF(wordDict, bow):  #\n",
    "    tfDict = {}  #new dictionary\n",
    "    bowCount = len(bow)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(bowCount)  #determining the frequency relative to other words of each unique word in our dataset\n",
    "    return tfDict\n",
    "\n",
    "tfbowA = computeTF(wordDictA,arta_stemmed)\n",
    "tfbowB = computeTF(wordDictB,artb_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T19:04:50.815788Z",
     "start_time": "2019-07-09T19:04:50.798363Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'court': 0.005434782608695652,\n",
       " 'com': 0.005434782608695652,\n",
       " 'come': 0.0,\n",
       " 'secretari': 0.0,\n",
       " 'europ': 0.005434782608695652,\n",
       " 'financ': 0.0,\n",
       " 'first': 0.005434782608695652,\n",
       " 'moratorium': 0.0,\n",
       " 'said': 0.010869565217391304,\n",
       " 'one': 0.016304347826086956,\n",
       " 'propos': 0.010869565217391304,\n",
       " 'disast': 0.0,\n",
       " 'hold': 0.005434782608695652,\n",
       " 'small': 0.010869565217391304,\n",
       " 'favour': 0.005434782608695652,\n",
       " 'rule': 0.005434782608695652,\n",
       " 'canada': 0.0,\n",
       " 'sourc': 0.005434782608695652,\n",
       " 'support': 0.010869565217391304,\n",
       " 'tsunami': 0.0,\n",
       " 'talk': 0.0,\n",
       " 'compani': 0.005434782608695652,\n",
       " 'affect': 0.0,\n",
       " 'affair': 0.005434782608695652,\n",
       " 'line': 0.005434782608695652,\n",
       " 'european': 0.010869565217391304,\n",
       " 'countri': 0.0,\n",
       " 'largest': 0.005434782608695652,\n",
       " 'happen': 0.005434782608695652,\n",
       " 'g': 0.0,\n",
       " 'briton': 0.0,\n",
       " 'problem': 0.0,\n",
       " 'bank': 0.0,\n",
       " 'controversi': 0.005434782608695652,\n",
       " 'parliament': 0.010869565217391304,\n",
       " 'minist': 0.0,\n",
       " 'instruct': 0.0,\n",
       " 'us': 0.016304347826086956,\n",
       " 'lead': 0.005434782608695652,\n",
       " 'firm': 0.010869565217391304,\n",
       " 'pressur': 0.005434782608695652,\n",
       " 'year': 0.0,\n",
       " 'implement': 0.010869565217391304,\n",
       " 'mep': 0.010869565217391304,\n",
       " 'suffer': 0.005434782608695652,\n",
       " 'law': 0.02717391304347826,\n",
       " 'earlier': 0.0,\n",
       " 'method': 0.005434782608695652,\n",
       " 'softwar': 0.016304347826086956,\n",
       " 'without': 0.005434782608695652,\n",
       " 'read': 0.005434782608695652,\n",
       " 'similar': 0.005434782608695652,\n",
       " 'jack': 0.0,\n",
       " 'reach': 0.0,\n",
       " 'final': 0.0,\n",
       " 'eu': 0.021739130434782608,\n",
       " 'biggest': 0.0,\n",
       " 'night': 0.0,\n",
       " 'click': 0.005434782608695652,\n",
       " 'repay': 0.0,\n",
       " 'abstain': 0.005434782608695652,\n",
       " 'play': 0.005434782608695652,\n",
       " 'analysi': 0.0,\n",
       " 'agre': 0.0,\n",
       " 'monetari': 0.0,\n",
       " 'expect': 0.0,\n",
       " 'shop': 0.005434782608695652,\n",
       " 'draft': 0.016304347826086956,\n",
       " 'model': 0.005434782608695652,\n",
       " 'fail': 0.005434782608695652,\n",
       " 'start': 0.005434782608695652,\n",
       " 'mr': 0.0,\n",
       " 'program': 0.005434782608695652,\n",
       " 'intern': 0.0,\n",
       " 'vocal': 0.005434782608695652,\n",
       " 'suspend': 0.0,\n",
       " 'intend': 0.005434782608695652,\n",
       " 'adopt': 0.005434782608695652,\n",
       " 'serv': 0.005434782608695652,\n",
       " 'current': 0.005434782608695652,\n",
       " 'submit': 0.005434782608695652,\n",
       " 'critic': 0.010869565217391304,\n",
       " 'word': 0.005434782608695652,\n",
       " 'would': 0.016304347826086956,\n",
       " 'direct': 0.021739130434782608,\n",
       " 'lock': 0.0,\n",
       " 'foreign': 0.0,\n",
       " 'reject': 0.005434782608695652,\n",
       " 'open': 0.005434782608695652,\n",
       " 'larger': 0.005434782608695652,\n",
       " 'implic': 0.005434782608695652,\n",
       " 'wealthi': 0.0,\n",
       " 'idea': 0.0,\n",
       " 'face': 0.0,\n",
       " 'deal': 0.0,\n",
       " 'welcom': 0.005434782608695652,\n",
       " 'member': 0.010869565217391304,\n",
       " 'base': 0.010869565217391304,\n",
       " 'group': 0.0,\n",
       " 'dead': 0.0,\n",
       " 'develop': 0.005434782608695652,\n",
       " 'gordon': 0.0,\n",
       " 'comput': 0.021739130434782608,\n",
       " 'gain': 0.005434782608695652,\n",
       " 'complet': 0.0,\n",
       " 'chancellor': 0.0,\n",
       " 'mean': 0.005434782608695652,\n",
       " 'fight': 0.005434782608695652,\n",
       " 'invent': 0.02717391304347826,\n",
       " 'setback': 0.005434782608695652,\n",
       " 'meet': 0.005434782608695652,\n",
       " 'exampl': 0.005434782608695652,\n",
       " 'chair': 0.0,\n",
       " 'field': 0.005434782608695652,\n",
       " 'chanc': 0.005434782608695652,\n",
       " 'number': 0.0,\n",
       " 'fund': 0.0,\n",
       " 'fear': 0.005434782608695652,\n",
       " 'ineffici': 0.005434782608695652,\n",
       " 'order': 0.010869565217391304,\n",
       " 'twice': 0.005434782608695652,\n",
       " 'two': 0.010869565217391304,\n",
       " 'say': 0.016304347826086956,\n",
       " 'momentum': 0.005434782608695652,\n",
       " 'nation': 0.005434782608695652,\n",
       " 'lobbi': 0.005434782608695652,\n",
       " 'might': 0.005434782608695652,\n",
       " 'pound': 0.0,\n",
       " 'put': 0.005434782608695652,\n",
       " 'straw': 0.0,\n",
       " 'legal': 0.016304347826086956,\n",
       " 'offer': 0.005434782608695652,\n",
       " 'new': 0.010869565217391304,\n",
       " 'rewrit': 0.005434782608695652,\n",
       " 'germani': 0.0,\n",
       " 'commiss': 0.005434782608695652,\n",
       " 'debt': 0.0,\n",
       " 'immens': 0.005434782608695652,\n",
       " 'debat': 0.005434782608695652,\n",
       " 'let': 0.005434782608695652,\n",
       " 'financi': 0.005434782608695652,\n",
       " 'back': 0.010869565217391304,\n",
       " 'impact': 0.005434782608695652,\n",
       " 'patent': 0.02717391304347826,\n",
       " 'brown': 0.0,\n",
       " 'union': 0.005434782608695652,\n",
       " 'amazon': 0.005434782608695652,\n",
       " 'internet': 0.005434782608695652,\n",
       " 'oppon': 0.005434782608695652,\n",
       " 'servic': 0.005434782608695652,\n",
       " 'use': 0.005434782608695652,\n",
       " 'state': 0.010869565217391304,\n",
       " 'issu': 0.005434782608695652,\n",
       " 'concern': 0.005434782608695652,\n",
       " 'announc': 0.0,\n",
       " 'bn': 0.0,\n",
       " 'miss': 0.0,\n",
       " 'hammer': 0.0,\n",
       " 'reconstruct': 0.0,\n",
       " 'give': 0.005434782608695652,\n",
       " 'fuller': 0.005434782608695652,\n",
       " 'hit': 0.0,\n",
       " 'thursday': 0.0,\n",
       " 'save': 0.0,\n",
       " 'hurt': 0.005434782608695652,\n",
       " 'later': 0.0,\n",
       " 'bring': 0.005434782608695652,\n",
       " 'japan': 0.0,\n",
       " 'decis': 0.005434782608695652,\n",
       " 'hope': 0.0,\n",
       " 'could': 0.016304347826086956,\n",
       " 'busi': 0.005434782608695652,\n",
       " 'also': 0.0,\n",
       " 'intens': 0.005434782608695652,\n",
       " 'vote': 0.005434782608695652,\n",
       " 'agreement': 0.0,\n",
       " 'effect': 0.005434782608695652,\n",
       " 'protect': 0.010869565217391304,\n",
       " 'innov': 0.005434782608695652,\n",
       " 'britain': 0.0,\n",
       " 'govern': 0.005434782608695652,\n",
       " 'larg': 0.005434782608695652,\n",
       " 'freez': 0.0,\n",
       " 'poland': 0.005434782608695652,\n",
       " 'begun': 0.0,\n",
       " 'believ': 0.0,\n",
       " 'juri': 0.010869565217391304,\n",
       " 'achiev': 0.005434782608695652,\n",
       " 'world': 0.0,\n",
       " 'interest': 0.0,\n",
       " 'even': 0.005434782608695652,\n",
       " 'thought': 0.0,\n",
       " 'month': 0.005434782608695652,\n",
       " 'committe': 0.010869565217391304,\n",
       " 'friday': 0.0,\n",
       " 'week': 0.0,\n",
       " 'reboot': 0.005434782608695652,\n",
       " 'permit': 0.005434782608695652,\n",
       " 'sign': 0.0,\n",
       " 'creditor': 0.0,\n",
       " 'action': 0.005434782608695652}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfbowA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T19:04:51.557013Z",
     "start_time": "2019-07-09T19:04:51.543629Z"
    }
   },
   "outputs": [],
   "source": [
    "def computeIDF(docList):  # creating frequency only if word is in dataset\n",
    "    import math\n",
    "    \"\"\"computer inverse doc freq for each doc in the docList\n",
    "    returns: IDF for each doc\n",
    "    \"\"\"\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / float(val))\n",
    "        \n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T19:04:52.151936Z",
     "start_time": "2019-07-09T19:04:52.140867Z"
    }
   },
   "outputs": [],
   "source": [
    "idfs = computeIDF([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T19:04:53.131321Z",
     "start_time": "2019-07-09T19:04:53.116110Z"
    }
   },
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T19:04:53.607176Z",
     "start_time": "2019-07-09T19:04:53.598701Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidfBowA = computeTFIDF(tfbowA, idfs)\n",
    "tfidfBowB = computeTFIDF(tfbowB, idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T19:04:53.918151Z",
     "start_time": "2019-07-09T19:04:53.803179Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstain</th>\n",
       "      <th>achiev</th>\n",
       "      <th>action</th>\n",
       "      <th>adopt</th>\n",
       "      <th>affair</th>\n",
       "      <th>affect</th>\n",
       "      <th>agre</th>\n",
       "      <th>agreement</th>\n",
       "      <th>also</th>\n",
       "      <th>amazon</th>\n",
       "      <th>...</th>\n",
       "      <th>vocal</th>\n",
       "      <th>vote</th>\n",
       "      <th>wealthi</th>\n",
       "      <th>week</th>\n",
       "      <th>welcom</th>\n",
       "      <th>without</th>\n",
       "      <th>word</th>\n",
       "      <th>world</th>\n",
       "      <th>would</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>0.005845</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    abstain    achiev    action     adopt    affair    affect      agre  \\\n",
       "0  0.001636  0.001636  0.001636  0.001636  0.001636  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.002923  0.002923   \n",
       "\n",
       "   agreement      also    amazon    ...        vocal      vote   wealthi  \\\n",
       "0   0.000000  0.000000  0.001636    ...     0.001636  0.001636  0.000000   \n",
       "1   0.002923  0.005845  0.000000    ...     0.000000  0.000000  0.002923   \n",
       "\n",
       "       week    welcom   without      word     world  would      year  \n",
       "0  0.000000  0.001636  0.001636  0.001636  0.000000    0.0  0.000000  \n",
       "1  0.002923  0.000000  0.000000  0.000000  0.002923    0.0  0.002923  \n",
       "\n",
       "[2 rows x 201 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame([tfidfBowA, tfidfBowB])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But yes, there is an easier way\n",
    "\n",
    "![big deal](https://media0.giphy.com/media/xUA7aQOxkz00lvCAOQ/giphy.gif?cid=3640f6095c2d7c51772f47644d09cc8b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T19:04:55.057637Z",
     "start_time": "2019-07-09T19:04:54.934837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    abstain    achiev    action     adopt    affair    affect      agre  \\\n",
      "0  0.053285  0.053285  0.053285  0.053285  0.053285  0.000000  0.000000   \n",
      "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.084167  0.084167   \n",
      "\n",
      "   agreement      also    amazon    ...        vocal      vote   wealthi  \\\n",
      "0   0.000000  0.000000  0.053285    ...     0.053285  0.053285  0.000000   \n",
      "1   0.084167  0.168334  0.000000    ...     0.000000  0.000000  0.084167   \n",
      "\n",
      "       week    welcom   without      word     world     would      year  \n",
      "0  0.000000  0.053285  0.053285  0.053285  0.000000  0.113738  0.000000  \n",
      "1  0.084167  0.000000  0.000000  0.000000  0.084167  0.059885  0.084167  \n",
      "\n",
      "[2 rows x 200 columns]\n"
     ]
    }
   ],
   "source": [
    "# create a string again\n",
    "cleaned_a = ' '.join(arta_stemmed)\n",
    "cleaned_b = ' '.join(artb_stemmed)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "response = tfidf.fit_transform([cleaned_a, cleaned_b])\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(response.toarray(), columns=tfidf.get_feature_names())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Statistics \n",
    "\n",
    "How many non-zero elements are there?\n",
    "- Adapt the code below, using the `df` version of the `response` object to replace everywhere below it says `DATA`\n",
    "- Interpret the findings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T19:04:55.869124Z",
     "start_time": "2019-07-09T19:04:55.858231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Number of Non-Zero Elements in Vectorized Articles: 103.5\n",
      "Percentage of columns containing 0: 0.48250000000000004\n"
     ]
    }
   ],
   "source": [
    "# Edit code before running it\n",
    "import numpy as np\n",
    "\n",
    "newval=np.array(df)\n",
    "\n",
    "non_zero_cols = np.count_nonzero(newval) / float(df.shape[0])\n",
    "print(\"Average Number of Non-Zero Elements in Vectorized Articles: {}\".format(non_zero_cols))\n",
    "\n",
    "percent_sparse = 1 - (non_zero_cols / float(df.shape[1]))\n",
    "print('Percentage of columns containing 0: {}'.format(percent_sparse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps:\n",
    "- Create the tf-idf for the **whole** corpus of 12 articles\n",
    "- What are _on average_ the most important words in the whole corpus?\n",
    "- Add a column named \"Target\" to the dataset\n",
    "- Target will be set to 1 or 0 if the article is \"Politics\" or \"Not Politics\"\n",
    "- Do some exploratory analysis of the dataset\n",
    " - what are the average most important words for the \"Politics\" articles?\n",
    " - What are the average most important words for the \"Not Politics\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets talk classification\n",
    "- How would you split into train and test? what would be the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T19:04:57.290159Z",
     "start_time": "2019-07-09T19:04:57.260317Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-3401c524cc6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Sample code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Sample code\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T19:04:59.155686Z",
     "start_time": "2019-07-09T19:04:57.772853Z"
    }
   },
   "outputs": [],
   "source": [
    "cleaned=[]\n",
    "for x in ['A', \"B\", 'C']:\n",
    "    url = f\"https://github.com/aapeebles/text_examples/blob/master/Text%20examples%20folder/{x}.txt\"\n",
    "    article_a= urllib.request.urlopen(url_a).read()\n",
    "    article_a_st = article_a.decode(\"utf-8\")\n",
    "    \n",
    "    # tokens\n",
    "    pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "    arta_tokens_raw = nltk.regexp_tokenize(article_a_st, pattern)\n",
    "\n",
    "    # lower case\n",
    "    arta_tokens = [i.lower() for i in arta_tokens_raw]\n",
    "\n",
    "    # stop words\n",
    "    from nltk.corpus import stopwords\n",
    "    stopwords.words(\"english\")\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    arta_tokens_stopped = [w for w in arta_tokens if not w in stop_words]\n",
    "\n",
    "    # stem words\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    arta_stemmed = [stemmer.stem(word) for word in arta_tokens_stopped]\n",
    "    \n",
    "    cleaned.append(arta_stemmed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T18:56:20.866914Z",
     "start_time": "2019-07-09T18:56:20.827811Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T19:05:01.486328Z",
     "start_time": "2019-07-09T19:05:01.470088Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-fb91987b33ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcleaned_list\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "cleaned_list= ','.join(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
